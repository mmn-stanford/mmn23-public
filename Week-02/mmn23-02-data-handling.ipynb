{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling for movies \n",
    "\n",
    "In cognitive neuroscience, what was considered impossible a few decades ago is now doable: Today, we can infer certain aspects of a person's cognitive processes from measurements of brain activity. This progress has come about due to a confluence of improvements in three different areas: computing speeds, brain imaging methods, and efficient machine learning algorithms. To use all three of these aspects for our data analysis, in an optimal manner, involves learning a complex sequence of steps. Our overarching goal for the problem sets in this course is to provide a step-by-step walk-through on how to execute these analysis steps and infer cognitive states from brain activity.\n",
    "\n",
    "This process begins with running experiments and collecting fMRI data. Then, collected data undergo preprocessing, which involves motion correction, cropping, transformations, and alignments. Only after these steps have been completed are the data ready for analysis by machine learning algorithms. Preprocessed fMRI data are commonly stored in the NIfTI format, the starting point for our analyses.\n",
    "\n",
    "In this notebook, our aim is to cover the very first step in the analysis after preprocessing: the extraction and normalization, and masking of pre-processed fMRI data. We will also do a basic analysis which will show you the potential of movie based analyses\n",
    "\n",
    "We will use our *first* movie dataset, called **Sherlock** from Chen et al. (2017). You can see more info about it in the [section](#movie) below.\n",
    "\n",
    "### Goal of this notebook\n",
    "This notebook will teach you how to visualize and normalize your data. Specifically, you will learn how to do the following:\n",
    " 1. Load fMRI data into Python.\n",
    " 2. Visualize fMRI data\n",
    " 3. Plot the timeseries for a voxel\n",
    " 4. Learn how masking volumes works\n",
    " 5. Assess the homotopic activity during movie watching\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "[1. Import necessary packages](#import)  \n",
    "\n",
    "[2. Introduction to the dataset](#movie)\n",
    "\n",
    "[3. BIDS formatted data](#bids)\n",
    "\n",
    "[4. Load fMRI](#load_fmri)  \n",
    "\n",
    "[5. Masking data](#masking)\n",
    "\n",
    "[6. Homotopic analyses](#homotopy)\n",
    "\n",
    "\n",
    "### Exercises\n",
    ">[Exercise 1](#ex1)   [2](#ex2)  [3](#ex3)  [4](#ex4)  [5](#ex5)  [6](#ex6)  [7](#ex7)  [8](#ex8)  [9](#ex9)  \n",
    ">[Novel contribution](#novel) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary packages <a id=\"import\"></a> \n",
    "\n",
    "Like we covered last week, we need to import the functions we will use. In the script today, we are importing from a script called `utils.py` (stored in your git repo) instead of spelling out the packages we will use. The utils file contains 3 types of information:\n",
    "1. The packages we will import. The main ones are listed below  \n",
    "> numpy: Perform numerical operations in Python.  \n",
    "> nibabel: Read  fMRI data into Python arrays.   \n",
    "> scipy: Scientific computing methods.  \n",
    "> nilearn: Used to extract masked fMRI data from a NIfTI file.  \n",
    "> sklearn: Machine learning methods.  \n",
    "2. Variables that are used often, like the paths to the data directories  \n",
    "3. Functions that are used often  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add the utils to the path\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# We want to import the contents of utils so that all of the variables are available (rather than accessible by utils.X).\n",
    "# \"import *\" does just that, it imports everything into the base\n",
    "from utils import * \n",
    "\n",
    "# Specify some jupyter magics. Specifcally, make plots non-interactive and save regularly\n",
    "%matplotlib inline \n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study:** You can navigate to the `utils` file in your jupyter notebook and read it in your browser. You should do that to get a feel for what it contains!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Sometimes we will have <strong>self-study</strong> sections that we encourage you to pursue ideas or read documents. These are intended to flesh out your understanding of the material and are encouraged.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the data directory for Sherlock, where we will pull data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data dir = %s' % (sherlock_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**<a id=\"ex1\"></a> Describe the difference in functionality between `import numpy` and `from numpy import zeros`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Sherlock <a id=\"movie\"></a> \n",
    "\n",
    "When you collect your own data, you will likely know it intimately; however, when you're analyzing data that someone else has collected, it can be easy to lose the details. The Sherlock dataset is one we will come back to multiple times throughout this course, so it is important to know it well. \n",
    "\n",
    "This data was collected for the experiment reported in [Chen et al. (2017)](https://doi.org/10.1038/nn.4450). Subjects were watching the first hour of [A Study in Pink](https://en.wikipedia.org/wiki/A_Study_in_Pink), henceforth called \"Sherlock\". Below is the abstract: \n",
    "\n",
    "> Our lives revolve around sharing experiences and memories with others. When different people recount the same events, how similar are their underlying neural representations? Participants viewed a 50-min movie, then verbally described the events during functional MRI, producing unguided detailed descriptions lasting up to 40 min. As each person spoke, event-specific spatial patterns were reinstated in default-network, medial-temporal, and high-level visual areas. Individual event patterns were both highly discriminable from one another and similar among people, suggesting consistent spatial organization. In many high-order areas, patterns were more similar between people recalling the same event than between recall and perception, indicating systematic reshaping of percept into memory. These results reveal the existence of a common spatial organization for memories in high-level cortical areas, where encoded information is largely abstracted beyond sensory constraints, and that neural patterns during perception are altered systematically across people into shared memory representations for real-life events.\n",
    "\n",
    "See a clip from the episode below. **Warning**: it contains gruesome content at 55s in. Watching it is entirely optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically we import all our functions at the top, but this one is pretty unique, so we are just doing it here.\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('VaT7IYQgyqo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sherlock data is stored in the `/farmshare/home/classes/psych/236/data/Sherlock/` folder.  This data was taken from [OpenNeuro](https://openneuro.org/datasets/ds001132/versions/00003) and so is stored in the [BIDS](https://bids.neuroimaging.io/), the current standard for MRI data storage (see below for more). In today's class, we are going to mainly focus on the `derivatives` folder, which contains data that has been preprocessed in useful ways. \n",
    "\n",
    "You have permission to view and read (AKA load) the files but you don't have permission to edit them. Hence, don't worry about altering the data through your analyses or exploration. \n",
    "\n",
    "The data has been preprocessed so that it is ready for analysis. Specifically, it has been aligned to standard space, making it easy for us to use for masking.\n",
    "\n",
    "Throughout this course, we are going to be relying on the movie watching (AKA perception) data, rather than the recall data that comes with Sherlock. The data was collected in two segments (AKA runs, since they stopped the scanner at the end of a segment) but they were combined in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:**<a id=\"ex2\"></a> To make sure you understand the data, we are going to ask you some questions about it. You should either examine the data in the directory or you should read the abstract/methods of the paper to get these answers:\n",
    "\n",
    "How many participants are there?  \n",
    "**A:** \n",
    "\n",
    "How long was each TR?  \n",
    "**A:** \n",
    "\n",
    "How big was each voxel in millimeters?  \n",
    "**A:**\n",
    "\n",
    "How long was the first segment in minutes? (Segment is a specific term from the paper)  \n",
    "**A:** \n",
    "\n",
    "Why is \"Let's all go to the lobby\" relevant to this dataset?  \n",
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The BIDS format <a id=\"bids\"></a>\n",
    "\n",
    "The [BIDS standard](https://bids.neuroimaging.io) for neuroimaging data is being increasingly used to share data across laboratories. It enables open science practices and reproducibility. Instead of using arbitrary names for files, the BIDS format enforces a file naming structure, making it easy for other people to understand the various files. \n",
    "\n",
    "The Sherlock data is stored in a BIDS directory. In the Sherlock directory there are the participant folders, stored by name (e.g., `sub-01`) and in each folder is their data organized by:\n",
    ">`func`: This folder is where all the functional neuroimaging data (epi) is stored.   \n",
    ">`anat`: Stores the structural data. \n",
    "\n",
    "Keywords that are part of the file name:\n",
    ">`task`: This label typically describes what the experiment was about.  \n",
    ">`bold`: This suffix denotes the type of data that is contained in the file. For fmri data it is 'bold'.  \n",
    "\n",
    "Using the above elements we can construct the file name. Once you have generated the file name, you can use Nilearn's data loading functions to read in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_bids='sub-01' # the subject id of the subject\n",
    "#Using the above information, we can get the fMRI data file location.\n",
    "file_loc= os.path.join(sherlock_dir,subj_bids,'func')\n",
    "print('Directory where files are stored: %s' % file_loc)\n",
    "\n",
    "#To get a specific file, we will need to add the following.\n",
    "task_bids='sherlockPart1' # the task name. change this for the specific dataset that you are using.\n",
    "bids_file_name=  '%s_task-%s' % (subj_bids, task_bids) + '_bold.nii.gz'\n",
    "\n",
    "print('BIDS file name: %s' % bids_file_name)\n",
    "\n",
    "bids_full_path_to_file= os.path.join(file_loc, bids_file_name)\n",
    "print('Full path to file: %s' % bids_full_path_to_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** <a id=\"ex3\"></a>Explain why a standardized file system is helpful for scientific reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load fMRI data <a id=\"load_fmri\"></a>\n",
    "\n",
    "We are now going to load in the movie data from a participant for one run. We will then use this data to consider various types of data handling.  As mentioned above, we are not using the raw Sherlock data today, but instead preprocessed data that is stored in the derivatives.\n",
    "\n",
    "The best way to load and save fMRI data with Python is to use nibabel (aka nib). The first step to load in data is to create an object that contains the header information. We will interrogate that header, and then do the second step: use that object to pull out the data. Below we do both steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the file name? We are getting participant 1\n",
    "file = sherlock_dir + '/derivatives/movie_files/sub-01.nii.gz'\n",
    "\n",
    "# Create the nifti object that serves as a header\n",
    "func_nii = nib.load(file)\n",
    "\n",
    "# Get the dimensionality of the data\n",
    "func_dim = func_nii.shape\n",
    "\n",
    "# Get the functional voxel size\n",
    "func_voxel = func_nii.header.get_zooms()\n",
    "\n",
    "# Report some details about the data\n",
    "print('The dimensionality of the data (x, y, z, TR): ' + str(func_dim))\n",
    "print('The size of the voxels (x, y, and z are in millimeters, TR is in seconds): ' + str(func_voxel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong> Recommendation:</strong>  Because Python suppresses output (when there is no error), you may want to include print statements at the end of cells to let you know when a cell has executed all lines of code. This also is a useful technique to debug your programs. \n",
    "    \n",
    "In Jupyter, there is an indicator to show that a cell is running - the asterisk '*' on the left hand side of the cell. Once the cell execution is complete, this changes to a number.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data volume\n",
    "print('Loading fMRI data, this will take a minute')\n",
    "func_vol = func_nii.get_fdata()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Be aware of exceeding your data allocation!</strong> When we started the jupyter notebook you are running, we requested a certain amount of memory resources. This amount is designed to be enough to do various operations, however, you will naturally be prohibited from running data intensive analyses. \n",
    "    \n",
    "fMRI data is high dimensional: there are 150000-300000 voxels per volume, and each voxel is usually stored with single precision 16-bit sign integers. However, numpy works with data in float 64 format, which is even larger. Thus, each voxel 'takes up' 8 bytes of computer memory (you can check this by running `func_vol.itemsize`). \n",
    "    \n",
    "For the data you just loaded, there were 271623 voxels per volume and 1976 TRs, which takes up 4.3 GB when uncompressed. If we loaded several participants and held all the data in memory, we would soon exhaust the data limit. When that happens, the notebook kernel will crash.\n",
    "\n",
    "Even though this file is 4.3 GB when loaded into memory, the data only takes up 0.9GB when stored in memory. That is because we use the compression codec gzip to compress the file without reducing data quality.    \n",
    "    \n",
    "*Fortunately*, you shouldn't lose your progress when you exceed the memory: You can just restart the notebook kernel. Nonetheless be careful loading in lots of fMRI data.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**<a id=\"ex4\"></a>: Visualize an axial slice of the first TR, halfway along the z-axis. If you need help doing this, refer to the code from week 1 where we did something similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different intensities on this plot reflect the Z-scored intensities for this first volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of great tools for exploring data that aren't just looking at individual slice like you just made. Here is one example tool that uses nilearn, another very useful tool for neuroimaging and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first TR\n",
    "print('Loading functional volume to just grab the first TR')\n",
    "epi_nii = nilearn.image.index_img(file, 0) # the 0 means take just the first TR\n",
    "\n",
    "# Plot the data as a 3d volume\n",
    "plotting.plot_stat_map(epi_nii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That isn't all though! We can make these plots interactive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an interactive plot of your data\n",
    "plotting.view_img(epi_nii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, you can click on the image and move the crosshairs. In this case, the coordinates at the bottom are the coordinates in standard space, measuring the distance in millimeters from the center of the brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just looked at the data in terms of *space*, so now lets look at the data in *time*. fMRI data is a 4d volume where the 4th dimension is time. This means we can just index a voxel and see the timecourse\n",
    "\n",
    "**Exercise 5:<a id=\"ex5\"></a>** Plot the timecourse of a voxel. Remember that we loaded the functional data into the 4d matrix `func_vol`. Using that, plot the timecourse of the voxel in the middle of all of the brain (Hint: if the brain were 60 x 80 x 100, plot the timecourse of voxel [30, 40, 50]). Remember to label your axes with meaningful names and the correct units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Masking<a id=\"masking\"></a>\n",
    "\n",
    "One of the key operations we can do on our data is to mask it so that we are only testing the voxels we care about. Functional acquisitions are large volumes that are usually only 30% brain, so it is important we ignore all of the non-brain. Moreover, in our analyses we often care about specific regions of the brain, rather than the brain as a whole.\n",
    "\n",
    "We are going to examine two ways of masking. The first way is to use numpy tools, treating the data like any array we manipulate with numpy. This is powerful and flexible and good enough for all uses. The second way is more specific to neuroimaging analyses, and comes with a few helpful features that relate to MRI data. \n",
    "\n",
    "The masks we are going to use are regions of the visual cortex from the Wang atlas (AKA the Kastner atlas) which comes from [this paper](https://pubmed.ncbi.nlm.nih.gov/25452571/) that is stored in `/farmshare/home/classes/psych/236/atlases/ProbAtlas_v4/`. This atlas divides the visual cortex into about 2 dozen areas (e.g., V1, V2, hMT, IPS1, FEF, etc.). We are working in volume space, so we will use the masks in the `subj_vol_all` folder, but there are also surface masks (surface space is an important format for fMRI data but no one we will cover in this course). For each area there is a volume that is numbered. The legend for the numbers is found in the `ROIfiles_Labeling.txt` file that is in the folder, see below for the output. There is a separate volume for each hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the legend for the \n",
    "!cat $atlas_path/ProbAtlas_v4/ROIfiles_Labeling.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Wait a minute!</strong> What just happened in that command above!? It has weird syntax that doesn't look like Python. \n",
    "    \n",
    "Well it wasn't Python, it was shell scripting. Shell script is the language used to interface with linux computers. One of the reasons Jupyter is so cool is that it lets you use shell scripting (or matlab or R) in the same environment as python. Variables even transfer over! \n",
    "    \n",
    "So let us break down the command:<br>\n",
    "`!`: This tells the notebook that the following command will be in shell script<br>\n",
    "`cat`: This is a shell script command for printing the content of files.<br>\n",
    "`$atlas_path`: The `atlas_path` variable is a Python variable (it was made in the `utils` file) and we are able to make the shell script recognize it as a variable by putting the `$` in front. This is really useful!!<br>\n",
    "`/ProbAtlas_v4/ROIfiles_Labeling.txt`: This is the name of file relative to the path.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more detail about masking: the masks from the atlas are in percentiles, meaning that each voxel reports the confidence that the voxel belongs in the mask (read the paper for more details). For masks to be useful, we need them to be binary. In other words, they must denote whether a voxel is in the mask or outside of the mask. Hence we are going to **threshold** the mask and say that any voxels above the threshold are in, any voxels below the threshold are out. For our purpose, our threshold will be 0%, meaning any voxels will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the mask file\n",
    "roi_counter = 7 # This is the number that corresponds to the ROI we will use (from the ROIfiles_Labeling file)\n",
    "mask_threshold = 0 # What is the percentile threshold for the mask that will be applied\n",
    "\n",
    "# Specify the mask file, this is a left hemisphere\n",
    "mask_file = '%s/ProbAtlas_v4/subj_vol_all/perc_VTPM_vol_roi%d_lh.nii.gz' % (atlas_path, roi_counter) # This is an important syntax for making strings in python, make sure you understand it\n",
    "\n",
    "# Load the mask header\n",
    "mask_nii = nib.load(mask_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the mask using our viewing tools. We are going to set the maximum value to the mask threshold so that the mask is just binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(mask_nii, \n",
    "                  threshold=mask_threshold, \n",
    "                  vmax=mask_threshold,\n",
    "                  colorbar=False,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking method 1: numpy \n",
    "We are now going to apply the mask to our functional data so that we only have voxels from inside the mask. For this method, our mask will be treated as a 3d volume and we will use that to mask a 4d volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mask as a volume\n",
    "mask_vol = mask_nii.get_fdata()\n",
    "\n",
    "print('Mask dimensionality:', mask_nii.shape)\n",
    "print('Voxel size:', mask_nii.header.get_zooms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just loaded in the mask and looked at the dimensionality. Compare the dimensionality to the dimensionality of the functional data reported [here](#load_fmri).  \n",
    "\n",
    "Hopefully you will notice that the volume is not the same dimensionality as the functional we loaded before. That means that if we want to make these two arrays interact then we need to change their dimensionality. There are lots of tools for doing this, like FSL, but fortunately nibabel gives us some great tools too. \n",
    "\n",
    "nibabel will only work if the data is aligned in the same space but has different voxel sizes. That is is the case for this data (if you want to make sure, divide each dimension by 3 and round a little). You should *NEVER* use these tools when the data is not aligned: the code will *work* but the results will be meaningless.. \n",
    "\n",
    "To make nibabel resize the data, we need the dimensionality of the data we want to align the mask to. We also need the voxel size of the data we want to align to. We have already got this information, stored in variables called `func_dim` and `func_voxel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command to align the data. Also see how Python lets you put line breaks within arguments to make it more readible!\n",
    "mask_aligned_nii = processing.conform(mask_nii, \n",
    "                                      out_shape=[func_dim[0], func_dim[1], func_dim[2]], \n",
    "                                      voxel_size=(func_voxel[0], func_voxel[0], func_voxel[0]),\n",
    "                                     )\n",
    "\n",
    "# Pull the volume data\n",
    "mask_aligned_vol = mask_aligned_nii.get_fdata()\n",
    "\n",
    "print('Mask dimensionality:', mask_aligned_nii.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self study:** We transformed the mask so that it is the same dimensionality as the functional data, but we could do the opposite: make the functional data the same dimensionality as the mask. Consider how you might do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:<a id=\"ex6\"></a>** Visualize the resized mask using the nilearn viewing tools, in conjunction with the functional data. Specifically, use the EPI as the `bg_img` and the mask as target image. If you aren't sure what `bg_img` is, read the help of the nilearn tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to threshold the mask and report how many voxels are included in the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_aligned_vol = mask_aligned_vol > mask_threshold\n",
    "print('Number of voxels in the mask', mask_aligned_vol.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything, we can mask the functional data. Python makes this very easy, as you will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_masked = func_vol[mask_aligned_vol == 1] # Take all the voxels equal to one\n",
    "print('Dimensionality of the data:', func_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data is now the number of voxels in the mask by the number of TRs. This is the format we want for masking, and the data is now very usable for other analyses.\n",
    "\n",
    "To go back to volume space after masking the data, we have to do the same kind of command but in reverse. You might do this when you alter/process the data in some way (e.g., you made a whole brain mask, applied it to the 4d volume to get a voxel by time array, z scored the array and then want to put it back into brain space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is effectively saying, for all the values in the mask that equal to 1, fill in the corresponding value from the masked data\n",
    "func_vol[mask_aligned_vol == 1] = func_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study**: What we did in that last step is surprisingly tricky. The mask is 3d but the functional is 4d, yet python, unlike a coding language like Matlab, knew how to interface the two.  This is because of a great feature of numpy, that would be worth the time to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking method 2: nilearn \n",
    "This route is a bit different and keeps the tools within nilearn. The first step is to threshold the mask. Then we make the masker object which can be applied to functional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data via nliearn (which is different but related to nibabel)\n",
    "mask_img = nilearn.image.load_img(mask_aligned_nii)\n",
    "\n",
    "# Threshold the data using nibabel math (which is a bit clunky)\n",
    "mask_img = nilearn.image.math_img('img > %s' % mask_threshold, img=mask_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the masker object\n",
    "nifti_masker = NiftiMasker(mask_img=mask_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the masker object to the nifti data, this will take some time. \n",
    "print('Masking nifti, this will take a minute')\n",
    "func_masked = nifti_masker.fit_transform(func_nii)\n",
    "\n",
    "print('Dimensionality of the data:', func_masked.shape)\n",
    "\n",
    "# You can even do this all in one step, like the code below:\n",
    "#func_masked = nilearn.masking.apply_mask(func_nii, mask_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the data is the same dimensionality but transposed, a quirk of this method to be wary of!\n",
    "\n",
    "To go back into volume space, you can use the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unmask the data\n",
    "unmasked_func = nilearn.masking.unmask(func_masked, mask_img)\n",
    "\n",
    "print('Dimensionality of the data:', func_masked.shape)\n",
    "print('Number of non-zero voxels per volume: %d' % np.sum(unmasked_func.get_fdata()[:, :, :, 0] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** due to the way nilearn works, this only puts the functional data into volume space, it doesn't put it back into the volume it came from. In other words, the only voxels in the volume are those from the mask. This is why there are only 548 non-zero voxels in this new mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:<a id=\"ex7\"></a>** Describe one reason or scenario you think the first masking method (numpy) is better, and another scenario where you think the second (nilearn) is better. There are lots of answers here, but you will need to think carefully!\n",
    "\n",
    "*Hint: for one of the reasons, consider memory demands.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking is a surprisingly important skill to have, so you will see that we will return to it a few times throughout the course. Make sure you understand it well now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Homotopic analyses<a id=\"homotopy\"></a>\n",
    "\n",
    "So we have done all these steps for data handling, lets now do something cool with it! Specifically, we are going to test the degree to which the functional activity is similar in the same visual region across areas. This homotopic analysis was originally introduced by [Biswal and colleagues (1995)](https://onlinelibrary.wiley.com/doi/10.1002/mrm.1910340409) and serves as a good test of whether functional activity is due to noise or is driven by signal. This is because the regions are separated by anatomical distance, so any similarity is likely due to shared driving activity.\n",
    "\n",
    "To do this we are going to mask the left and right side of a visual region (in our case, hV4) and then apply that mask to our functional data. This gives us two arrays of voxel by time. We will then average across voxels to get a timecourse of activity. Finally, we will correlate the two timecourses to see how related they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the ROI you are using\n",
    "roi_counter = 7\n",
    "\n",
    "hemi_timecourses = {} # Preset array to store the data\n",
    "plt.figure() #Set up the figure\n",
    "for hemi in ['lh', 'rh']:\n",
    "    \n",
    "    # Specify the ROI name you will use to mask\n",
    "    roi_name = 'roi%d_%s' % (roi_counter, hemi)\n",
    "    \n",
    "    # Specify the mask file, this is a left hemisphere\n",
    "    mask_file = '%s/ProbAtlas_v4/subj_vol_all/perc_VTPM_vol_%s.nii.gz' % (atlas_path, roi_name) # This is an important syntax for making strings in python, make sure you understand it\n",
    "\n",
    "    # Load the mask header\n",
    "    mask_nii = nib.load(mask_file)\n",
    "    \n",
    "    # Run the command to align the data to our functional\n",
    "    mask_aligned_nii = processing.conform(mask_nii, \n",
    "                                          out_shape=[func_dim[0], func_dim[1], func_dim[2]], \n",
    "                                          voxel_size=(func_voxel[0], func_voxel[0], func_voxel[0]),\n",
    "                                         )\n",
    "\n",
    "    # Pull the mask data and threshold it\n",
    "    mask_aligned_vol = mask_aligned_nii.get_fdata() > mask_threshold\n",
    "    \n",
    "    # Mask the functional data\n",
    "    func_masked = func_vol[mask_aligned_vol == 1]\n",
    "    \n",
    "    # Average the data along the voxel axis\n",
    "    hemi_timecourse = np.mean(func_masked, axis=0)\n",
    "    \n",
    "    # Plot the first 100 TRs of the timecourse\n",
    "    plt.plot(hemi_timecourse[:100])\n",
    "    \n",
    "    # Store the timecourse\n",
    "    hemi_timecourses[roi_name] = hemi_timecourse\n",
    "    \n",
    "# Specify the figure parameters    \n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('Intensity (Z)')\n",
    "plt.title('Similarity of timecourse between hemispheres for the first 100 TRs')\n",
    "plt.legend(['left hemi', 'right hemi']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the similarity of the timecourses\n",
    "lh_timecourse = hemi_timecourses['roi%d_lh' % roi_counter]\n",
    "rh_timecourse = hemi_timecourses['roi%d_rh' % roi_counter]\n",
    "\n",
    "r_val = np.corrcoef(lh_timecourse, rh_timecourse)[0, 1]\n",
    "\n",
    "print('Correlation between left and right hemisphere for ROI %d: %0.2f' % (roi_counter, r_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! That is a pretty high correlation! Even though the left and right hemispheres are far apart, the fluctuations in activity between the left and right hV4 are almost identical! This is a good sign that these regions are responding strongly to the movie. So even though those fluctuations look kind of noisy, they seem to contain real signal related to the participant watching the movie. In a way, this is the very essence of the course and shows the potential that movies have for fMRI analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:<a id=\"ex8\"></a>** Hold on a minute! This analysis is misleading because it suggests that the two regions are highly correlated *because* of the movie, but they could be correlated for other reasons. For instance, these regions might have a high correlation at rest when there is no movie, suggesting that the movie isn't evoking activity. Another reason this correlation could be misleading is that motion could be a confound. Speculate on a way that you think motion could cause this correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9<a id=\"ex9\"></a>:** One way to test whether this correlation is capturing signal is to compare this correlation we just got with a non-homotopic correlation. For instance, you could correlate left LO1 with right hV4 to test if that non-homotopic correlation is lower. \n",
    "\n",
    "In the code below, report the correlation between right hV4 and left LO1. Then in the section after it, interpret that correlation (i.e., why this is informative?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret results here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now finished the main part of the problem set, but now it is time for extra credit. Remember that you can get up to 10% more on your problem set grade by completing an **optional** novel contribution. To give you an idea of what you could do, focus on *part a* of the figure below. This is from [Arcaro and Livingstone (2017)](https://elifesciences.org/articles/26196) and reports a homotopy analysis in which several regions from the left hemisphere are compared to each other, forming a matrix (shown on the left). Although it seems complicated, the code above only needs to be edited a little to make this kind of plot.\n",
    "\n",
    "If you make a novel contribution that involves this level of insight or creativity then you will be rewarded with extra credit. However, you should know that fewer than half of the class will get extra credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://iiif.elifesciences.org/lax/26196%2Felife-26196-fig2-v1.tif/full/1500,/0/default.jpg >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put novel contribution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a>\n",
    "\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 01/2018  \n",
    "T. Meissner minor edits  \n",
    "Q. Lu switch to matplotlib, color blind friendly colors, encapsulate helper functions, del ex.3 (loop)  \n",
    "M. Kumar: Added Exercise 10, deleted masking exercise.  \n",
    "K.A. Norman provided suggestions on the overall content and made edits to this notebook.  \n",
    "C. Ellis: Incorporated comments from cmhn-s19.   \n",
    "A.K. Sahoo made minor edits to the notebook.  \n",
    "Q. Lu consistent plotting format; slightly modified the exs; adding solutions.  \n",
    "T. Yates edits for cmhn-s21  \n",
    "C. Ellis edits for mmn23"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
